Task: Build a Generative Search Engine for My Local Files Using Llama 3

System Design: 
  1. An index with the content of the local files, with an information retrieval engine to retrieve the most relevant documents for a given query/question.
  2. A language model to use selected content from local documents and generate a summarized answer
  3. A user interface

How the Components Interact: 
~ Qdrant creates index from local files
~ The user asks Streamlit a question 
~ Streamlit retrieves relative document chunks 
~ Streamlit passes the user's question with top retrieved documents to Llama 3 and intructions L3 to generate an answer
~ L3 generates an answer to Streamlit 
~ Streamlit returns answer to user

Building a Semantic Index: 
  What is a Semantic Index? Why do we need one? A semantic index that will provide us with the most relevant documents based on the similarity of the file's content and a given query.
  We use Qdrant as a vector store to create such an index. 

Step 1: Create VE 
  `python3 -m venv myev`
  `source myev/bin/activate`

Step 2: Install Qdrant Client and Other Necessary Packages
  `pip install qdrant-client` # Install Qdrant client for semantic indexing
  `pip install transformers` # Install language model libraries from Hugging Face
  `pip install streamlit` # Install Streamlit for the UI
  `pip install fastapi uvicorn` # Install FastAPI and Uvicorn for the web service
  `pip install python-docx PyPDF2 python-pptx` # Install additional libraries for handling documents

  OR you can use this line of code: 
    `pip install streamlit fastapi uvicorn qdrant-client transformers python-docx PyPDF2 python-pptx`

Step 3: Creating a Vector Index
  "In order to create a vector index, we will have to embed the documents on the hard drive." 
  "For embeddings, we will have to select the right embedding method and the right vector comparison metric."

  We need an embedding model that will work well with asymmetric search problems. 
  "Asymmetric search problems are common to information retrieval and happen when one has short queries and long documents."

  "Embedding models fine-tuned on the MSMARCO dataset usually work well."
  "MSMARCO dataset is based on Bing Search queries and documents and has been released by Microsoft. Therefore, it is ideal for the problem we are dealing with."

  The model we are using uses dot product as the similarity metric instead of cosine similarity. 
  The model: sentence-transformers/msmarco-bert-base-dot-v5

Step 3 and 4: Generative Search API and User Interface

Luckily, the article we are using as a guide has attached a github. This will help with creating the index.
So, lets clone the repo: https://github.com/nikolamilosevic86/local-genAI-search
  git clone https://github.com/nikolamilosevic86/local-gen-search.git
  cd local-gen-search
  pip install -r requirements.txt

Then, we must create a create a file called environment_var.py and put my huggingface API key there. 

file: 
  import os
  hf_token = "hf_you_api_key"
  nvidia_key = "nvapi-your_nvidia_nim_api_key"

I also need to request access to Llama3 model at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
NOTE!!! AT THIS POINT I ABANDONED THE PROJECT. WILL COME BACK WHEN MY REQUESTED ACCESS TO LLAMA 3 ON HUGGING FACE IS GRANTED.

Task 2: 

